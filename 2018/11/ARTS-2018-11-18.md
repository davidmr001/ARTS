
 [Algorithm ] (#algorithm)
 algo

 [Review](#review)

 [Technique spark2.1版本中遇到的一个bug](#technique)

 [Share Mac系统更换brew源的方法](#share)


# Algorithm


# Review


# Technique




今天生产环境的一个spark导出数据到ES的任务，被反映执行时间过长，平时用时在半小时左右，今天已经执行了3个半小时，还没结束。

上去看了一下，是一个读取hive表数据导入到ES的任务，整个表的数据量1亿+，大小约15G。

spark任务执行关键参数是：10个executor，每个executor分配5G内存，并使用5个core。

第一时间先去看Yarn的log是否有异常（spark运行在yarn上），进入监控页面之后，发现并无异常，但看到有一个task，耗时不正常，其他的任务都花费5,6分钟就结束了，这个task运行了近3个多小时，第一时间想到是否是数据倾斜导致的？但看了shuffle读取的数据量与其他的task读取的大小差不多，然后接着看这个任务的log，发现并无异常，这种现象挺奇怪的，没有异常也没有数据倾斜，为何处理时间花的这么久？

随后，观察es的数据量，发现并无增长变化，这个时候怀疑任务是不是死锁了，导致整个Job阻塞，迟迟结束不了。  点击spark监控页面上的Executors面板，可以看当前Executor（JVM进程）的Thread Dump的日志，点击进去之后，发现一个线程为Executor task launch worker-10的线程log如下：

```
sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:309)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:54)
scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:66)
org.elasticsearch.spark.sql.EsSparkSQL$$anonfun$saveToEs$1.apply(EsSparkSQL.scala:94)
org.elasticsearch.spark.sql.EsSparkSQL$$anonfun$saveToEs$1.apply(EsSparkSQL.scala:94)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
org.apache.spark.scheduler.Task.run(Task.scala:99)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
java.lang.Thread.run(Thread.java:745)
```

注意上面日志中的，这个类：

```
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:309)
```

ShuffleBlockFetcherIterator类里的next方法，调用了LinkedBlockingQueue的take方法，导致这个worker线程一直在等待，阻塞队列LinkedBlockingQueue如果是空的时候，去调用take方法，会阻塞调用线程，这是阻塞队列最基本的特征，所以这个应该是不是根本原因，继续看线程的dump文件，发现有一个线程名为shuffle-client-5-1的线程有点问题，其执行代码状态一直处于某个方法，这就奇怪了，这个地方没有任何的同步和锁的服务，或者操作系统的挂起命令，但一直处于该初，说明应该是陷入死循环了，如下：

```
io.netty.util.Recycler$Stack.scavengeSome(Recycler.java:504)
io.netty.util.Recycler$Stack.scavenge(Recycler.java:454)
io.netty.util.Recycler$Stack.pop(Recycler.java:435)
io.netty.util.Recycler.get(Recycler.java:144)
io.netty.buffer.PooledUnsafeDirectByteBuf.newInstance(PooledUnsafeDirectByteBuf.java:39)
io.netty.buffer.PoolArena$DirectArena.newByteBuf(PoolArena.java:727)
io.netty.buffer.PoolArena.allocate(PoolArena.java:140)
io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:271)
io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:177)
io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:168)
io.netty.buffer.AbstractByteBufAllocator.ioBuffer(AbstractByteBufAllocator.java:129)
io.netty.channel.AdaptiveRecvByteBufAllocator$HandleImpl.allocate(AdaptiveRecvByteBufAllocator.java:104)
io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:117)
io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652)
io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
java.lang.Thread.run(Thread.java:745)
```


这个线程是属于netty的，这里简单介绍一下，spark在做shuffer的时候是通过netty来传输数据的，shuffer的时候会在map侧启动shuffer-server提供server服务，然后reduce，启动shuff-client客户端，去服务端拉取数据汇总，这个地方是循环链表的一个操作，但一时半会看不出什么问题，心想是个问题，看看网上是否已经有朋友遇到过，在google上一搜，果然有几个关于spark executor执行任务hang住的jira的bug，最后找到了下面的issue：

https://issues.apache.org/jira/browse/SPARK-18971
https://github.com/netty/netty/issues/6153

根本是netty的问题，在链表循环那个方法里面，会有几率出现链表闭链的bug，一旦形成循环链表就会hang住任务，从而导致整个Job无法执行完。具体的描述，大家可参考上面的jira连接。

影响的版本，理论上说只要是Netty4.0.43.Final之前的版本，所有的依赖netty的该版本的框架，都有几率触发。

解决办法：

升级spark2.1.0为 2.1.2,  2.1.3,或者2.2.0的版本






# Share



如果在使用brew安装软件的时候，特别慢，可以重新换个brew的下载源，有点类似yum的机制：

如果已经卡住了，请按Ctrl+C退出，mac上注意看好是control，别按成Command+C了，我就犯了这低级错误
。

```
# 进入brew主目录
$ cd `brew --repo`

# 更换镜像
$ git remote set-url origin https://git.coding.net/homebrew/homebrew.git

# 测试效果
$ brew update
```

其他的源：

```
https://git.coding.net/homebrew/homebrew.git - Coding
https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git - 清华
https://mirrors.ustc.edu.cn/brew.git - 中科大
```




