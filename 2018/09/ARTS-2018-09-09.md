
 [Algorithm](#algorithm)

 [Review：理解Java7和8里面HashMap+ConcurrentHashMap的扩容策略](#review)
 
 [Technique：理解Java8并发工具类ConcurrentHashMap的实现](#technique)
 
 [Share：理解另类的并发安全实现CopyOnWriteArrayList ](#share)


# Algorithm


# Review


### 前言

理解HashMap和ConcurrentHashMap的重点在于：

（1）理解HashMap的数据结构的设计和实现思路

（2）在（1）的基础上，理解ConcurrentHashMap的并发安全的设计和实现思路

前面的文章已经介绍过Map结构的底层实现，这里我们重点放在其扩容方法，
这里分别对JDK7和JDK8版本的HashMap+ConcurrentHashMap来分析：


### JDK7的HashMap扩容

这个版本的HashMap数据结构还是数组+链表的方式，扩容方法如下：

```
void transfer(Entry[] newTable) {  
    Entry[] src = table;                   //src引用了旧的Entry数组  
    int newCapacity = newTable.length;  
    for (int j = 0; j < src.length; j++) { //遍历旧的Entry数组  
        Entry<K, V> e = src[j];             //取得旧Entry数组的每个元素  
        if (e != null) {  
            src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象）  
            do {  
                Entry<K, V> next = e.next;  
                int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置  
                e.next = newTable[i]; //标记[1]  
                newTable[i] = e;      //将元素放在数组上  
                e = next;             //访问下一个Entry链上的元素  
            } while (e != null);  
        }  
    }  
}
```

上面的这段代码不并不难理解，对于扩容操作，底层实现都需要新生成一个数组，然后拷贝旧数组里面的每一个Node链表到新数组里面，这个方法在单线程下执行是没有任何问题的，但是在多线程下面却有很大问题，主要的问题在于基于头插法的数据迁移，会有几率造成链表倒置，从而引发链表闭链，导致程序死循环，并吃满CPU。据说已经有人给原来的SUN公司提过bug，但sun公司认为，这是开发者使用不当造成的，因为这个类本就不是线程安全的，你还偏在多线程下使用，这下好了吧，出了问题这能怪我咯？仔细想想，还有点道理。



### JDK7的ConcurrentHashMap扩容

HashMap是线程不安全的，我们来看下线程安全的ConcurrentHashMap，在JDK7的时候，这种安全策略采用的是分段锁的机制，ConcurrentHashMap维护了一个Segment数组，Segment这个类继承了重入锁ReentrantLock，并且该类里面维护了一个
HashEntry<K,V>[] table数组，在写操作put，remove，扩容的时候，会对Segment加锁，所以仅仅影响这个Segment，不同的Segment还是可以并发的，所以解决了线程的安全问题，同时又采用了分段锁也提升了并发的效率。
![image](http://pic.yupoo.com/goldendoc/Ba4GCFe1/nuEZ0.png)

下面看下其扩容的源码：

```
// 方法参数上的 node 是这次扩容后，需要添加到新的数组中的数据。
private void rehash(HashEntry<K,V> node) {
    HashEntry<K,V>[] oldTable = table;
    int oldCapacity = oldTable.length;
    // 2 倍
    int newCapacity = oldCapacity << 1;
    threshold = (int)(newCapacity * loadFactor);
    // 创建新数组
    HashEntry<K,V>[] newTable =
        (HashEntry<K,V>[]) new HashEntry[newCapacity];
    // 新的掩码，如从 16 扩容到 32，那么 sizeMask 为 31，对应二进制 ‘000...00011111’
    int sizeMask = newCapacity - 1;

    // 遍历原数组，老套路，将原数组位置 i 处的链表拆分到 新数组位置 i 和 i+oldCap 两个位置
    for (int i = 0; i < oldCapacity ; i++) {
        // e 是链表的第一个元素
        HashEntry<K,V> e = oldTable[i];
        if (e != null) {
            HashEntry<K,V> next = e.next;
            // 计算应该放置在新数组中的位置，
            // 假设原数组长度为 16，e 在 oldTable[3] 处，那么 idx 只可能是 3 或者是 3 + 16 = 19
            int idx = e.hash & sizeMask;
            if (next == null)   // 该位置处只有一个元素，那比较好办
                newTable[idx] = e;
            else { // Reuse consecutive sequence at same slot
                // e 是链表表头
                HashEntry<K,V> lastRun = e;
                // idx 是当前链表的头结点 e 的新位置
                int lastIdx = idx;

                // 下面这个 for 循环会找到一个 lastRun 节点，这个节点之后的所有元素是将要放到一起的
                for (HashEntry<K,V> last = next;
                     last != null;
                     last = last.next) {
                    int k = last.hash & sizeMask;
                    if (k != lastIdx) {
                        lastIdx = k;
                        lastRun = last;
                    }
                }
                // 将 lastRun 及其之后的所有节点组成的这个链表放到 lastIdx 这个位置
                newTable[lastIdx] = lastRun;
                // 下面的操作是处理 lastRun 之前的节点，
                //    这些节点可能分配在另一个链表中，也可能分配到上面的那个链表中
                for (HashEntry<K,V> p = e; p != lastRun; p = p.next) {
                    V v = p.value;
                    int h = p.hash;
                    int k = h & sizeMask;
                    HashEntry<K,V> n = newTable[k];
                    newTable[k] = new HashEntry<K,V>(h, p.key, v, n);
                }
            }
        }
    }
    // 将新来的 node 放到新数组中刚刚的 两个链表之一 的 头部
    int nodeIndex = node.hash & sizeMask; // add the new node
    node.setNext(newTable[nodeIndex]);
    newTable[nodeIndex] = node;
    table = newTable;
}
```
注意这里面的代码，外部已经加锁，所以这里面是安全的，我们看下具体的实现方式：先对数组的长度增加一倍，然后遍历原来的旧的table数组，把每一个数组元素也就是Node链表迁移到新的数组里面，最后迁移完毕之后，把新数组的引用直接替换旧的。此外这里这有一个小的细节优化，在迁移链表时用了两个for循环，第一个for的目的是为了，判断是否有迁移位置一样的元素并且位置还是相邻，根据HashMap的设计策略，首先table的大小必须是2的n次方，我们知道扩容后的每个链表的元素的位置，要么不变，要么是原table索引位置+原table的容量大小，举个例子假如现在有三个元素（3,5,7）要放入map里面，table的的容量是2，简单的假设元素位置=元素的值 % 2，得到如下结构：

```
[0]=null
[1]=3->5->7
```

现在将table的大小扩容成4，分布如下：
```
[0]=null
[1]=5->7
[2]=null
[3]=3
```
因为扩容必须是2的n次方，所以HashMap在put和get元素的时候直接取key的hashCode然后经过再次均衡后直接采用&位运算就能达到取模效果，这个不再细说，上面这个例子的目的是为了说明扩容后的数据分布策略，要么保留在原位置，要么会被均衡在旧的table位置，这里是1加上旧的table容量这是是2，所以是3。基于这个特点，第一个for循环，作的优化如下，假设我们现在用0表示原位置，1表示迁移到index+oldCap的位置，来代表元素：
```
[0]=null
[1]=0->1->1->0->0->0->0
```
第一个for循环的会记录lastRun，比如要迁移[1]的数据，经过这个循环之后，lastRun的位置会记录第三个0的位置，因为后面的数据都是0，代表他们要迁移到新的数组中同一个位置中，所以就可以把这个中间节点，直接插入到新的数组位置而后面附带的一串元素其实都不需要动。

接着第二个循环里面在此从第一个0的位置开始遍历到lastRun也就是第三个元素的位置就可以了，只循环处理前面的数据即可，这个循环里面根据位置0和1做不同的链表追加，后面的数据已经被优化的迁移走了，但最坏情况下可能后面一个也没优化，比如下面的结构：

```
[0]=null
[1]=1->1->0->0->0->0->1->0
```

这种情况，第一个for循环没多大作用，需要通过第二个for循环从头开始遍历到尾部，按0和1分发迁移，这里面使用的是还是头插法的方式迁移，新迁移的数据是追加在链表的头部，但这里是线程安全的所以不会出现循环链表，导致死循环问题。迁移完成之后直接将最新的元素加入，最后将新的table替换旧的table即可。




### JDK8的HashMap扩容

在JDK8里面，HashMap的底层数据结构已经变为数组+链表+红黑树的结构了，因为在hash冲突严重的情况下，链表的查询效率是O(n），所以JDK8做了优化对于单个链表的个数大于8的链表，会直接转为红黑树结构算是以空间换时间，这样以来查询的效率就变为O(logN)，图示如下：

![image](https://www.javadoop.com/blogimages/map/2.png)

我们看下其扩容代码：

```
    final Node<K,V>[] resize() {
        Node<K,V>[] oldTab = table;
        int oldCap = (oldTab == null) ? 0 : oldTab.length;
        int oldThr = threshold;
        int newCap, newThr = 0;
        if (oldCap > 0) {
            if (oldCap >= MAXIMUM_CAPACITY) {
                threshold = Integer.MAX_VALUE;
                return oldTab;
            }
            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                     oldCap >= DEFAULT_INITIAL_CAPACITY)
                newThr = oldThr << 1; // double threshold
        }
        else if (oldThr > 0) // initial capacity was placed in threshold
            newCap = oldThr;
        else {               // zero initial threshold signifies using defaults
            newCap = DEFAULT_INITIAL_CAPACITY;
            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
        }
        if (newThr == 0) {
            float ft = (float)newCap * loadFactor;
            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE);
        }
        threshold = newThr;
        @SuppressWarnings({"rawtypes","unchecked"})
            Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
        table = newTab;
        if (oldTab != null) {
            for (int j = 0; j < oldCap; ++j) {
                Node<K,V> e;
                if ((e = oldTab[j]) != null) {
                    oldTab[j] = null;
                    if (e.next == null)
                        newTab[e.hash & (newCap - 1)] = e;
                    else if (e instanceof TreeNode)
                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                    else { 
                        //重点关注区域
                        // preserve order
                        Node<K,V> loHead = null, loTail = null;
                        Node<K,V> hiHead = null, hiTail = null;
                        Node<K,V> next;
                        do {
                            next = e.next;
                            if ((e.hash & oldCap) == 0) {
                                if (loTail == null)
                                    loHead = e;
                                else
                                    loTail.next = e;
                                loTail = e;
                            }
                            else {
                                if (hiTail == null)
                                    hiHead = e;
                                else
                                    hiTail.next = e;
                                hiTail = e;
                            }
                        } while ((e = next) != null);
                        if (loTail != null) {
                            loTail.next = null;
                            newTab[j] = loHead;
                        }
                        if (hiTail != null) {
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead;
                        }
                    }
                }
            }
        }
        return newTab;
    }

```
在JDK8中，单纯的HashMap数据结构增加了红黑树是一个大的优化，此外根据上面的迁移扩容策略，我们发现JDK8里面HashMap没有采用头插法转移链表数据，而是保留了元素的顺序位置，新的代码里面采用：

```
                        //按原始链表顺序，过滤出来扩容后位置不变的元素（低位=0），放在一起
                        Node<K,V> loHead = null, loTail = null;
                        //按原始链表顺序，过滤出来扩容后位置改变到（index+oldCap）的元素（高位=0），放在一起
                        Node<K,V> hiHead = null, hiTail = null;
```
把要迁移的元素分类之后，最后在分别放到新数组对应的位置上：

```
                        //位置不变    
                        if (loTail != null) {
                            loTail.next = null;
                            newTab[j] = loHead;
                        }
                        //位置迁移(index+oldCap)
                        if (hiTail != null) {
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead;
                        }
```

JDK7里面是先判断table的存储元素的数量是否超过当前的threshold=table.length*loadFactor（默认0.75），如果超过就先扩容，在JDK8里面是先插入数据，插入之后在判断下一次++size的大小是否会超过当前的阈值，如果超过就扩容。



### JDK8的ConcurrentHashMap扩容

在JDK8中彻底抛弃了JDK7的分段锁的机制，新的版本主要使用了Unsafe类的CAS自旋赋值+synchronized同步+LockSupport阻塞等手段实现的高效并发，代码可读性稍差。

ConcurrentHashMap的JDK8与JDK7版本的并发实现相比，最大的区别在于JDK8的锁粒度更细，理想情况下talbe数组元素的大小就是其支持并发的最大个数，在JDK7里面最大并发个数就是Segment的个数，默认值是16，可以通过构造函数改变一经创建不可更改，这个值就是并发的粒度，每一个segment下面管理一个table数组，加锁的时候其实锁住的是整个segment，这样设计的好处在于数组的扩容是不会影响其他的segment的，简化了并发设计，不足之处在于并发的粒度稍粗，所以在JDK8里面，去掉了分段锁，将锁的级别控制在了更细粒度的table元素级别，也就是说只需要锁住这个链表的head节点，并不会影响其他的table元素的读写，好处在于并发的粒度更细，影响更小，从而并发效率更好，但不足之处在于并发扩容的时候，由于操作的table都是同一个，不像JDK7中分段控制，所以这里需要等扩容完之后，所有的读写操作才能进行，所以扩容的效率就成为了整个并发的一个瓶颈点，好在Doug lea大神对扩容做了优化，本来在一个线程扩容的时候，如果影响了其他线程的数据，那么其他的线程的读写操作都应该阻塞，但Doug lea说你们闲着也是闲着，不如来一起参与扩容任务，这样人多力量大，办完事你们该干啥干啥，别浪费时间，于是在JDK8的源码里面就引入了一个ForwardingNode类，在一个线程发起扩容的时候，就会改变sizeCtl这个值，其含义如下：

```
sizeCtl ：默认为0，用来控制table的初始化和扩容操作，具体应用在后续会体现出来。
-1 代表table正在初始化
-N 表示有N-1个线程正在进行扩容操作
其余情况：
1、如果table未初始化，表示table需要初始化的大小。
2、如果table初始化完成，表示table的容量，默认是table大小的0.75倍
```
扩容时候会判断这个值，如果超过阈值就要扩容，首先根据运算得到需要遍历的次数i，然后利用tabAt方法获得i位置的元素f，初始化一个forwardNode实例fwd，如果f == null，则在table中的i位置放入fwd，否则采用头插法的方式把当前旧table数组的指定任务范围的数据给迁移到新的数组中，然后
给旧table原位置赋值fwd。直到遍历过所有的节点以后就完成了复制工作，把table指向nextTable，并更新sizeCtl为新数组大小的0.75倍 ，扩容完成。在此期间如果其他线程的有读写操作都会判断head节点是否为forwardNode节点，如果是就帮助扩容。

扩容源码如下：

```
    private final void transfer(Node<K,V>[] tab, Node<K,V>[] nextTab) {
        int n = tab.length, stride;
        if ((stride = (NCPU > 1) ? (n >>> 3) / NCPU : n) < MIN_TRANSFER_STRIDE)
            stride = MIN_TRANSFER_STRIDE; // subdivide range
        if (nextTab == null) {            // initiating
            try {
                @SuppressWarnings("unchecked")
                Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n << 1];
                nextTab = nt;
            } catch (Throwable ex) {      // try to cope with OOME
                sizeCtl = Integer.MAX_VALUE;
                return;
            }
            nextTable = nextTab;
            transferIndex = n;
        }
        int nextn = nextTab.length;
        ForwardingNode<K,V> fwd = new ForwardingNode<K,V>(nextTab);
        boolean advance = true;
        boolean finishing = false; // to ensure sweep before committing nextTab
        for (int i = 0, bound = 0;;) {
            Node<K,V> f; int fh;
            while (advance) {
                int nextIndex, nextBound;
                if (--i >= bound || finishing)
                    advance = false;
                else if ((nextIndex = transferIndex) <= 0) {
                    i = -1;
                    advance = false;
                }
                else if (U.compareAndSwapInt
                         (this, TRANSFERINDEX, nextIndex,
                          nextBound = (nextIndex > stride ?
                                       nextIndex - stride : 0))) {
                    bound = nextBound;
                    i = nextIndex - 1;
                    advance = false;
                }
            }
            if (i < 0 || i >= n || i + n >= nextn) {
                int sc;
                if (finishing) {
                    nextTable = null;
                    table = nextTab;
                    sizeCtl = (n << 1) - (n >>> 1);
                    return;
                }
                if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) {
                    if ((sc - 2) != resizeStamp(n) << RESIZE_STAMP_SHIFT)
                        return;
                    finishing = advance = true;
                    i = n; // recheck before commit
                }
            }
            else if ((f = tabAt(tab, i)) == null)
                advance = casTabAt(tab, i, null, fwd);
            else if ((fh = f.hash) == MOVED)
                advance = true; // already processed
            else {
                synchronized (f) {
                    if (tabAt(tab, i) == f) {
                        Node<K,V> ln, hn;
                        if (fh >= 0) {
                            int runBit = fh & n;
                            Node<K,V> lastRun = f;
                            for (Node<K,V> p = f.next; p != null; p = p.next) {
                                int b = p.hash & n;
                                if (b != runBit) {
                                    runBit = b;
                                    lastRun = p;
                                }
                            }
                            if (runBit == 0) {
                                ln = lastRun;
                                hn = null;
                            }
                            else {
                                hn = lastRun;
                                ln = null;
                            }
                            for (Node<K,V> p = f; p != lastRun; p = p.next) {
                                int ph = p.hash; K pk = p.key; V pv = p.val;
                                if ((ph & n) == 0)
                                    ln = new Node<K,V>(ph, pk, pv, ln);
                                else
                                    hn = new Node<K,V>(ph, pk, pv, hn);
                            }
                            setTabAt(nextTab, i, ln);
                            setTabAt(nextTab, i + n, hn);
                            setTabAt(tab, i, fwd);
                            advance = true;
                        }
                        else if (f instanceof TreeBin) {
                            TreeBin<K,V> t = (TreeBin<K,V>)f;
                            TreeNode<K,V> lo = null, loTail = null;
                            TreeNode<K,V> hi = null, hiTail = null;
                            int lc = 0, hc = 0;
                            for (Node<K,V> e = t.first; e != null; e = e.next) {
                                int h = e.hash;
                                TreeNode<K,V> p = new TreeNode<K,V>
                                    (h, e.key, e.val, null, null);
                                if ((h & n) == 0) {
                                    if ((p.prev = loTail) == null)
                                        lo = p;
                                    else
                                        loTail.next = p;
                                    loTail = p;
                                    ++lc;
                                }
                                else {
                                    if ((p.prev = hiTail) == null)
                                        hi = p;
                                    else
                                        hiTail.next = p;
                                    hiTail = p;
                                    ++hc;
                                }
                            }
                            ln = (lc <= UNTREEIFY_THRESHOLD) ? untreeify(lo) :
                                (hc != 0) ? new TreeBin<K,V>(lo) : t;
                            hn = (hc <= UNTREEIFY_THRESHOLD) ? untreeify(hi) :
                                (lc != 0) ? new TreeBin<K,V>(hi) : t;
                            setTabAt(nextTab, i, ln);
                            setTabAt(nextTab, i + n, hn);
                            setTabAt(tab, i, fwd);
                            advance = true;
                        }
                    }
                }
            }
        }
    }
```

### 在扩容时读写操作如何进行

(1)对于get读操作，如果当前节点有数据，还没迁移完成，此时不影响读，能够正常进行。

如果当前链表已经迁移完成，那么头节点会被设置成fwd节点，此时get线程会帮助扩容。


(2)对于put/remove写操作，如果当前链表已经迁移完成，那么头节点会被设置成fwd节点，此时写线程会帮助扩容，如果扩容没有完成，当前链表的头节点会被锁住，所以写线程会被阻塞，直到扩容完成。

### 对于size和迭代器是弱一致性

volatile修饰的数组引用是强可见的，但是其元素却不一定，所以，这导致size的根据sumCount的方法并不准确。

同理Iteritor的迭代器也一样，并不能准确反映最新的实际情况



### 总结

本文主要了介绍了HashMap+ConcurrentHashMap的扩容策略，扩容的原理是新生成大于原来1倍大小的数组，然后拷贝旧数组数据到新的数组里面，在多线程情况下，这里面如果注意线程安全问题，在解决安全问题的同时，我们也要关注其效率，这才是并发容器类的最出色的地方



# Technique

前面的文章已经分析过List和Queue相关的接口与并发实现类，本篇我们来分析一下非常Java里面非常重要的一个数据结构HashMap。（注意Set类型在这里我们不在单独分析，因为Set本身并不能算一种数据结构，它可以借助任何其他数据结构如array或者map类来实现。）

我们先看下Java里面一些常见的Map类型：

线程不安全的Map：
```
HashMap （允许key和value都为null）
TreeMap （允许value为null）
LinkedHashMap （允许key和value都为null）
```
线程安全的Map：

```
ConcurrentHashMap （key和value都不允许为null）
Hashtable (key和value都不允许为null)
Collections.synchronizedMap(map) (key和value都不允许为null)
```


在有序性方面：

HashMap是无序的，底层实现是数组+单向链表

LinkedHashMap可以保持插入顺序，底层实现是数组+双向链表

TreeMap是基于比较器顺序的，可以自定义key的排序规则，底层实现是数组+红黑树结构


上面的三个Map都是线程不安全的，也就是说在多线程下使用是有问题的，所以如果需要多线程场景下使用，就必须使用线程安全的集合，这里面Hashtable 和Collections.synchronizedMap(map)是JDK里面出现比较早的线程安全的map类，虽然是线程安全，但因为并不高效，因为其内部完全用的简单的synchronized来保证同步，在多线程竞争激烈情况下，效率较低。


不安全的Map在多线程下使用肯定是会有问题的，这毋庸置疑，比如JDK8之前的HashMap在高并发下如果有多个线程同时采用头插法扩容链表操作，那么将会有很大几率导致链表闭链，从而引发死循环导致CPU占满。这里简单分析一下原因：

看JDK7里面HashMap扩容的核心代码：

```
    void transfer(Entry[] newTable, boolean rehash) {
        int newCapacity = newTable.length;
        for (Entry<K,V> e : table) {
            while(null != e) {
                Entry<K,V> next = e.next;            ---------------------(1)
                if (rehash) {
                    e.hash = null == e.key ? 0 : hash(e.key);
                }
                int i = indexFor(e.hash, newCapacity); 
                e.next = newTable[i];
                newTable[i] = e;
                e = next;
            } // while
 
        }
    }
```

为了方便理解，我们制造如下的代码：

```
Map<Integer> map = new HashMap<Integer>(2);  
```
上面的代码现在我们只放置两个元素3和7，其中的threshold为1，所以当再次插入数据时候，map会进行扩容操作，扩容后的大小是原长度的2倍也就是4。

我们简化现在的存放策略是对table数组的长度取模，由于3和7模上2都等于1，所以都会放在table数组1的


```
[0]= null
[1]= 3->7 
```
如果再增加一个元素时候会发生扩容，数组的长度会变成4，然后数据需要迁移，迁移的方式是头插法，新加入的节点会插入链表的头部。

假设有两个线程同时进行扩容操作，第一个线程刚执行到下面这一步
```
Entry<K,V> next = e.next;  
```
第二个线程已经完成扩容，扩容后的格式如下：

```
[0]= null
[1]= 3->7
[2]= 7->3
[3]= null
```
然后第一个线程继续执行，第二个线程执行完会把数据刷新到主内存里面，注意JMM内存模型在这里没有可见性保证，因为第一个线程并不是在第二个线程关闭之后启动的，所以此时线程二的修改结果，对于线程一来说可能是可见的。如果是可见的。

在扩容时候，线程一table[1]的7后面的引用变成了3，在扩容后，table下标2的位置就会出现如下的情况：
```
[2]=3->7->3
```
这样就导致了基于头插法倒置的链表就出现了死循环。


在JDK8以来，对HashMap内部做了很大的改进，数据结构采用了数组+链表+红黑树的方法来存储元素，针对扩容操作，不在改变原来的table数组的数据结构，而是在基于复制的思想在新数组上进行改动，完成之后在切换引用，避免了死循环的问题，但其仍然不是线程安全的，比如在多线程put的时候会发生丢数据，对迭代器遍历的时候会发生fail-fast，所以针对这种情况才有了ConcurrentHashMap这种高效安全的并发工具类。


在JDK7中的ConcurrentHashMap采用了分段锁的技术，每个段类似一个独立的数组+链表结构，并发粒度控制在Segment级别，如下图：

 ![image](https://upload-images.jianshu.io/upload_images/7853175-5137bd1f4f444bde.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)


不难发现采用这种方式，并发粒度还是太粗了，对于同一个Segment下面不同的数组链表数，如果有多个线程访问仍然要等待，所以在jdk8中取消了分段锁的思想，改用基于CAS自旋+synchronized控制并发操作，实现了更粒度的锁控制。JDK8的源码里仍然保留了Segment类，仅仅是为了兼容旧的版本，不做其他的用途。


前面说过JDK8的ConcurrentHashMap用了数组+链表+红黑树的数据结构，如下图：
![image](https://javadoop.com/blogimages/map/2.png)

重要的成员字段：

```
// 核心数组存储
 transient volatile Node<K,V>[] table;
// 扩容时用到的数组
private transient volatile Node<K,V>[] nextTable;

/*用来控制table的初始化和扩容操作
#0：默认值
#-1：代表哈希表正在进行初始化
#大于0：相当于 HashMap 中的 threshold，表示阈值
#小于-1：代表有多个线程正在进行扩容
*/
private transient volatile int sizeCtl;
// 默认初始化table容量
private static final intDEFAULT_CAPACITY = 16;
// 默认的负载因子
private static final float LOAD_FACTOR = 0.75f;
// 链表转树的阀值，如果table[i]下面的链表长度大于8时就转化为红黑树
static final int TREEIFY_THRESHOLD = 8;
//树转链表的阀值，小于等于6是转为链表，仅在扩容tranfer时才可能树转链表
static final int UNTREEIFY_THRESHOLD = 6; 
```

核心的链表Node类：

```
    static class Node<K,V> implements Map.Entry<K,V> {
        final int hash;
        final K key;
        volatile V val;
        volatile Node<K,V> next;
        //.......其他省略
        }
```


链表转树时，并不会直接转，只是把这些节点包装成TreeNode放到TreeBin中， 
再由TreeBin来转化红黑树

```
    static final class TreeNode<K,V> extends Node<K,V> {
        TreeNode<K,V> parent;  // red-black tree links
        TreeNode<K,V> left;
        TreeNode<K,V> right;
        TreeNode<K,V> prev;    // needed to unlink next upon deletion
        boolean red;
        //.......
        }
```




TreeBin封装了TreeNode，当链表转树时，用于封装TreeNode，也就是说，ConcurrentHashMap的红黑树存放的时TreeBin，而不是treeNode。

```
    static final class TreeBin<K,V> extends Node<K,V> {
        TreeNode<K,V> root;
        volatile TreeNode<K,V> first;
        volatile Thread waiter;
        volatile int lockState;
        // values for lockState
        static final int WRITER = 1; // set while holding write lock
        static final int WAITER = 2; // set when waiting for write lock
        static final int READER = 4; // increment value for setting read lock
        //......
        }
```




特殊的Node节点ForwardingNode类：
作用：在扩容的时候插在链表的头部，用来标识状态
````
    static final class ForwardingNode<K,V> extends Node<K,V> {
        final Node<K,V>[] nextTable;
        ForwardingNode(Node<K,V>[] tab) {
            super(MOVED, null, null, null);
            this.nextTable = tab;
        }
    }
````



put(k,v)方法分析（源码不在贴出来）

（1）先判断key和value是否为null，为null扔出异常

（2）判断table是否初始化，如果没有则进行初始化

（3）计算key的hash值，并得到插入的数组索引。

（4）找到table[i]的位置，如果为null直接插入，如果不为null判断此key是否存在，如果存在直接覆盖，如果不存在进行判断
如果head节点是树节点，按照红黑树的方式插入新的节点，如果不是则按照链表的方式插入，同时会判断当前的链表长度是否大于8，如果大于则转为红黑树再插入，否则直接插入，插入采用的CAS自旋的方式。

（5）最后判断table的size是否需要扩容，如果需要则扩容，否则就结束。在扩容的时候会在链表头部插入forward，如果其他线程检测到需要插入的位置被forward节点占有，就帮助进行扩容。

get方法分析：

get方法比较简单，因为不涉及并发问题，直接就根据key的hash值定位到链表，然后遍历查询即可。


size方法分析：

计算节点数量，计算baseCount和CounterCell.value的总和


entrySet方法分析：

通过EntrySetView类提供了当前的map的视图，在当前视图上的remove操作可以直接映射到Map上，反之亦然，这个视图提供了弱一致性的保证，在遍历删除的时候不会出现 Fail-fast的并发修改异常。




总结：

本文主要介绍了Java8里面HashMap的相关内容并着重介绍了ConcurrentHashMap的实现和核心方法分析，HashMap是我们日常开发中使用频率最高的类之一，而ConcurrentHashMap则是在并发编程中的高效工具类，理解其实核心设计，则对我们的工作和学习有很大帮助。


















# Share


在Java的并发包java.util.concurrent里面有一个比较有意思现象，针对Map和LinkList都有对应的高效的+线程安全的并发实现类：
```
ConcurrentHashMap
ConcurrentLinkedQueue
```
唯独没有针对ArrayList的高效的并发实现，这个我们后面在细说，先来看下目前在Java里面线程安全的List有三种：
```
Vector 
Collections.synchronizedList(List list)
CopyOnWriteArrayList 
```
Vector这个类是一个非常古老的类了，在JDK1.0的时候便已经存在，其实现安全的手段非常简单所有的方法都加上synchronized关键字，这样保证这个实例的方法同一时刻只能有一个线程访问，所以在高并发场景下性能非常低。


Collections.synchronizedList(List list)这个方法，其实在内部有一个SynchronizedList包装类对应，其实现安全的手段稍微有一点优化，就是把Vector加在方法上的synchronized关键字，移到了方法里面变成了同步块而不是同步方法从而把锁的范围缩小了，而且在构造函数的时候可以传入不同的同步监视器，实现基于不同的监视器并发，但我觉得没有多大意义，只能保证一个监视器内是线程安全的，不同监视器还是不安全的，除非另一个是只读操作，但如果是这样，完全可以用并发包的读写锁来替代。

CopyOnWriteArrayList这个类比较特殊，对于写作来说是基于重入锁互斥的，对于读操作来说是无锁的。还有一个特殊的地方，这个类的iterator是fail-safe的，也就是说是线程安全List里面的唯一一个不会出现ConcurrentModificationException异常的类。能做到这一点其实是有代价的，跟它的实现机制有很大关系，我们从名字上就能看出来，CopyOnWriteArrayList这个类内部维护的核心内容：

```java
    //重入锁保写操作互斥
    final transient ReentrantLock lock = new ReentrantLock();
    //volatile保证读可见性
    private transient volatile Object[] array;
```

从上面的代码能够看出，为了实现无锁读，对象数组上面是加了volatile修饰的，当然如果你去掉这个关键字，那么对于读操作来说也是必须加锁的。volatile相比读加锁实现则更轻量级。

接着我们看这个类是如何添加数据的：

```
     
    public boolean add(E e) {
        final ReentrantLock lock = this.lock;
        lock.lock();//加锁
        try {
            Object[] elements = getArray();//读取原数组
            int len = elements.length;
            //构建一个长度为len+1的新数组，然后拷贝旧数据的数据到新数组
            Object[] newElements = Arrays.copyOf(elements, len + 1);
            //把新加的数据赋值到最后一位
            newElements[len] = e;
            // 替换旧的数组
            setArray(newElements);
            return true;
        } finally {
            lock.unlock();
        }
    }
```

简单的说，这个类每次新加的数据都会新copy生成一个数组来容纳，并不是直接修改原来的数据结构，这种方式提供了安全的快照读和遍历的方法，带来的不足就是对于频繁写的应用并不适合，Doug Lea大神在开发这个类的时候也介绍了这个类的主要应用场景是避免对集合的iterator方法加锁遍历。

这里要提一下对于对于Java里面的集合类无论是线程安全和不安全的，只要涉及到在遍历的时候修改数据，就会抛出异常，原因是集合类的modCount字段与Iteritor记录的expectedModCount字段值不相等，也就是不同步导致的，这是集合类的fail-fast机制，在单线程情况下我们可以通过Iteritor的remove方法来避免抛出ConcurrentModificationException异常，但在多线程情况下仍然是有问题的，如果想要解决，有两种方式：

（1）在遍历IterItor时候，采用加锁策略，避免多个线程同时修改。

（2）采用弱一致性的副本，原理是不改变原来数据，比如CopyOnWriteArrayList这种的。这种解决方法非常类似数据库技术的MVCC多版本的模式，对于Iteritor生成的时候，读取的是当前数组的快照，所以在遍历的时候永远不会存在ConcurrentModificationException异常，注意CopyOnWriteArrayList是不支持Iteritor.remove操作的，因为对快照的删除是没有任何意义的，所以想要删除必须调用CopyOnWriteArrayList.remove方法，前面说过该类的写操作相关的方法都是线程互斥的，所以不存在安全问题。



整体来说CopyOnWriteArrayList是另类的线程安全的实现，但并一定是高效的，适合用在读取和遍历多的场景下，并不适合写并发高的场景，因为数组的拷贝也是非常耗时的，尤其是数据量大的情况下。



到这里我们能够看到关于List的线程安全实现基本都是采用加锁实现，只不过CopyOnWriteArrayList是比较特殊的另类的安全并发实现，包括同样的CopyOnWriteArraySet（底层用的CopyOnWriteArrayList），这里强调了线程安全，但并没有提到高效，因为HashMap和LinkQueue都有对应的线程安全+高效的并发容器，只有List没有，主要原因如下：

在Java并发编程网有一篇关于这个的解释，我在这里总结一下：

在java.util.concurrent包中没有加入并发的ArrayList实现的主要原因是：很难去开发一个通用并且没有并发瓶颈的线程安全的List。


（1）ConcurrentHashMap这样的类的真正价值在于，在保证了线程安全的同时，采用了分段锁+弱一致性的Map迭代器提供了高效的并发效率。如果仅仅是线程安全而不高效，对于高并发来说意义不大。

（2）而ConcurrentLinkedQueue/ConcurrentLinkedDeque基于链表的高并发实现采用了CAS+自旋的方式，提供了无阻塞的高并发实现，主要原因是他们的接口相比List的接口有更多的限制，这些限制使得实现并发成为可能。


而ArrayList这样的数据结，不知道如何去规避并发的瓶颈，拿contains() 这样一个操作来说，当你进行搜索的时候如何避免锁住整个list？

CopyOnWriteArrayList的实现仅仅是规避了读并发的瓶颈，对于修改操作扔然是需要锁住整个List的，所以从某种程度上来说，实现一个通用高效的并发List是比较困难的，这也是java并发包里为什么没有该实现的原因。


本文主要介绍了Java并发包里面另类的安全实现方式CopyOnWriteArrayList的实现原理，其主要特点是利用了快照的概念从而使读和迭代器遍历操作无须同步加锁，由于其不可变的特性，所以在并发应用中更加容易处理，但这种方式也是有代价的毕竟数组的拷贝在数据大的时候也是一笔不少的开销，这一点需要注意。















